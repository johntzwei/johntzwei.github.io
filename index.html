<!DOCTYPE html>
<html lang="en">
   <head>
      <!-- Hi, this is Nelson. Please DELETE the <script> block below (L6-L13)
         if you use this HTML, otherwise my analytics will track your page. -->
      <title>Johnny Tian-Zheng Wei | 魏天正</title>
      <meta http-equiv="content-type" content="text/html; charset=UTF-8">
      <meta charset="utf-8">
      <meta property="og:url" content="http://johntzwei.github.io" />
      <meta property="og:title" content="Johnny Tian-Zheng Wei | 魏天正" />
      <meta property="og:image" content="pic" />
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="author" content="Johnny Tian-Zheng W">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <link rel="shortcut icon" type="image/png" href="favicon.ico"/>
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
      <link rel="stylesheet" href="css/style.css">
      <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
      <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
      <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
   </head>
   <body>
      <div class="container mt-5">
         <div class="row mb-3">
            <div class="col">
               <h1>Johnny Tian-Zheng Wei | 魏天正</h1>
            </div>
         </div>
         <div class="row">
            <div class="col-lg-4 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1">
               <div class="card mb-3">
                  <script>
                     var sites = [
                     "pdfs/a_negative_result.txt"
                     ];
                     
                     function randomSite() {
                     var i = parseInt(Math.random() * sites.length);
                     location.href = sites[i];
                     }
                  </script>
                  <a href="#" onclick="randomSite();"><img class="card-img-top" src="pic.jpg" alt="Johnny Tian-Zheng Wei"></a>
                  <div class="card-body">
                     <h5 class="card-title">
                        <b>Johnny Tian-Zheng Wei</b>
                     </h5>
                     <p class="card-text">
                        PhD Candidate
                        </br>
                        Computer Science Department
                        </br>
                        University of Southern California
                        </br>
                        </br>
                        Office: GCS 4th floor
                        </br>
                        Pronouns: he/him
                     </p>
                  </div>
               </div>
            </div>
            <div class="col-lg-8 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0">
               <p>
                  Hi! I am a PhD candidate (since fall 2019) at the <a href="https://www.usc.edu/"
                    >University of Southern California</a>, where I am advised by <a href="https://robinjia.github.io/"
                    >Robin Jia</a>. My current interest is in the legal issues of AI, and my work often takes basic concepts from inferential statistics. Previously, I earned a B.S. in Mathematics at the <a href="https://www.umass.edu/"
                    >University of Massachusetts Amherst</a>. When there are slow weeks at school, you might find me lifting (myself), swimming, biking, reading, writing, or learning. For all that I do, I want to do with love.
               </p>
               <style>
                  .video-container {
                  position: relative;
                  width: 100%;
                  padding-top: 56.25%; /* 16:9 Aspect Ratio */
                  }
                  .video-container iframe {
                  position: absolute;
                  top: 0;
                  left: 0;
                  width: 100%;
                  height: 100%;
                  }
               </style>
               <p>
                  Check out our <a href="https://sites.google.com/view/memorization-workshop/home">workshop on LLM memorization</a>!
               </p>
            </div>
         </div>
         <div class="row">
            <div class="col">
               <p>
                  Email: jtwei [<a href="https://en.wikipedia.org/wiki/At_sign">strudel</a>] usc.edu
               </p>
               <p>
                  Links:
                  [<a href="https://twitter.com/johntzwei">Twitter</a>] [<a href="https://github.com/johntzwei">Github</a>] [<a href="https://scholar.google.com/citations?user=TVFpiukAAAAJ">Google Scholar</a>]
               </p>
            </div>
         </div>
         <hr>
         <div class="row" id="publications">
            <div class="col">
               <h2>Selected publications</h2>
               A full list of my publications can be found on <a href="https://scholar.google.com/citations?user=TVFpiukAAAAJ">Google Scholar</a>.
               <br>
               <ul class="pl">
                  <li>
                     <a href="https://arxiv.org/pdf/2510.19811">
                     <b>Hubble: a model suite to advance the study of LLM memorization</b>
                     </a>
                     <br/>
                     <b>Johnny Tian-Zheng Wei</b>*,
                     <a href="https://ameyagodbole.github.io/">Ameya Godbole</a>*, 
                     <a href="https://aflah02.github.io/">Mohammad Aflah Khan</a>*,
                     <a href="https://ryanyxw.github.io/">Ryan Wang</a>,
                     <a href="https://xyzhu123.com/">Xiaoyuan Zhu</a>,
                     <a href="https://james-flemings.github.io/">James Flemings</a>,
                     <a href="https://www.linkedin.com/in/nitya-kashyap-bb119429a/">Nitya Kashyap</a>,
                     <a href="https://people.mpi-sws.org/~gummadi/">Krishna P. Gummadi</a>,
                     <a href="https://willieneis.github.io/">Willie Neiswanger</a>,
                     and <a href="https://robinjia.github.io/">Robin Jia</a>.
                     <br/>
                     ICLR 2026.
                     <br/>
                     [<a href="#" onclick="$('#hubble_abstract').toggle();return false;">abstract</a>]
                     [<a href="">bib</a>]
                     [<a href="https://github.com/allegro-lab/hubble">github</a>]
                     [<a href="https://allegro-lab.github.io/hubble/">website</a>]
                     <div id="hubble_abstract" class="abstract" style="display:none;">
                        <p>
                           We present Hubble, a suite of fully open-source large language models (LLMs) for the scientific study of LLM memorization. Hubble models come in standard and perturbed variants: standard models are pretrained on a large English corpus, and perturbed models are trained in the same way but with controlled insertion of text (e.g., book passages, biographies, and test sets) designed to emulate key memorization risks. Our core release includes 8 models -- standard and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B tokens -- establishing that memorization risks are determined by the frequency of sensitive data relative to size of the training corpus (i.e., a password appearing once in a smaller corpus is memorized better than the same password in a larger corpus). Our release also includes 6 perturbed models with text inserted at different pretraining phases, showing that sensitive data without continued exposure can be forgotten. These findings suggest two best practices for addressing memorization risks: to dilute sensitive data by increasing the size of the training corpus, and to order sensitive data to appear earlier in training. Beyond these general empirical findings, Hubble enables a broad range of memorization research; for example, analyzing the biographies reveals how readily different types of private information are memorized. We also demonstrate that the randomized insertions in Hubble make it an ideal testbed for membership inference and machine unlearning, and invite the community to further explore, benchmark, and build upon our work.
                        </p>
                     </div>
                  </li>
                  <br/>
                  <li>
                     <a href="https://arxiv.org/pdf/2502.16290">
                     <b>Interrogating LLM design under a fair learning doctrine</b>
                     </a>
                     <br/>
                     <b>Johnny Tian-Zheng Wei</b>*,
                     <a href="">Maggie Wang</a>*, 
                     <a href="https://ameyagodbole.github.io/">Ameya Godbole</a>,
                     <a href="https://www.jonathanhchoi.com/">Jonathan H. Choi</a>,
                     and <a href="https://robinjia.github.io/">Robin Jia</a>.
                     <br/>
                     <b>FAccT</b> 2025.
                     <br/>
                     [<a href="#" onclick="$('#fair_learning_abstract').toggle();return false;">abstract</a>]
                     [<a href="pdfs/facct2025.bib">bib</a>]
                     [<a href="https://www.youtube.com/watch?v=6Svoh1xu63k">video</a>]
                     <div id="fair_learning_abstract" class="abstract" style="display:none;">
                        <p>
                           The current discourse on large language models (LLMs) and copyright largely takes a "behavioral" perspective, focusing on model outputs and evaluating whether they are substantially similar to training data. However, substantial similarity is difficult to define algorithmically and a narrow focus on model outputs is insufficient to address all copyright risks. In this interdisciplinary work, we take a complementary "structural" perspective and shift our focus to how LLMs are trained. We operationalize a notion of "fair learning" by measuring whether any training decision substantially affected the model's memorization. As a case study, we deconstruct Pythia, an open-source LLM, and demonstrate the use of causal and correlational analyses to make factual determinations about Pythia's training decisions. By proposing a legal standard for fair learning and connecting memorization analyses to this standard, we identify how judges may advance the goals of copyright law through adjudication. Finally, we discuss how a fair learning standard might evolve to enhance its clarity by becoming more rule-like and incorporating external technical guidelines.
                        </p>
                     </div>
                  </li>
                  <br/>
                  <li>
                     <a href="https://arxiv.org/pdf/2402.10892.pdf">
                     <b>Proving membership in LLM pretraining data via data watermarks</b>
                     </a>
                     <br/>
                     <b>Johnny Tian-Zheng Wei</b>*,
                     <a href="https://ryanyxw.github.io/">Ryan Wang</a>*, 
                     and <a href="https://robinjia.github.io/">Robin Jia</a>.
                     <br/>
                     Findings of <b>ACL</b> 2024</a>.
                     <br/>
                     [<a href="#" onclick="$('#proving_membership_abstract').toggle();return false;">abstract</a>]
                     [<a href="https://github.com/ryanyxw/datawatermarks">github</a>]
                     [<a href="https://aclanthology.org/2024.findings-acl.788.bib">bib</a>]
                     <div id="proving_membership_abstract" class="abstract" style="display:none;">
                        <p>
                           Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B's training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.
                        </p>
                     </div>
                  </li>
                  <br/>
                  <li>
                     <a href="https://arxiv.org/pdf/2305.09601.pdf">
                     <b>Operationalizing content moderation "accuracy" in the Digital Services Act</b>
                     </a>
                     <br/>
                     <b>Johnny Tian-Zheng Wei</b>,
                     <a href="https://www.coll.mpg.de/frederike-zufall">Frederike Zufall</a>, 
                     and <a href="https://robinjia.github.io/">Robin Jia</a>.
                     <br/>
                     <b>AIES</b> 2024</a>.
                     <br/>
                     [<a href="#" onclick="$('#operationalizing_accuracy_abstract').toggle();return false;">abstract</a>]
                     [<a href="https://dblp.org/rec/conf/aies/WeiZJ24.html?view=bibtex">bib</a>]
                     [<a href="https://github.com/johntzwei/operationalizing_accuracy">github</a>]
                     <div id="operationalizing_accuracy_abstract" class="abstract" style="display:none;">
                        <p>
                           The Digital Services Act, recently adopted by the EU, requires social media platforms to report the "accuracy" of their automated content moderation systems. The colloquial term is vague, or open-textured -- the literal accuracy (number of correct predictions divided by the total) is not suitable for problems with large class imbalance, and the ground truth and dataset to measure accuracy against is unspecified. Without further specification, the regulatory requirement allows for deficient reporting. In this interdisciplinary work, we operationalize "accuracy" reporting by refining legal concepts and relating them to technical implementation. We start by elucidating the legislative purpose of the Act to legally justify an interpretation of "accuracy" as precision and recall. These metrics remain informative in class imbalanced settings while also reflecting the proportional balancing of Fundamental Rights of the EU Charter. We then focus on the estimation of recall, as its naive estimation can incur extremely high annotation costs and disproportionately interfere with the platform's right to conduct business. Through a simulation study, we show that recall can be efficiently estimated using stratified sampling with trained classifiers, and provide concrete recommendations for its application. Finally, we present a case study of recall reporting for a subset of Reddit under the Act. Based on the language in the Act, we identify a number of ways recall could be reported due to underspecification. We report on one possibility using our improved estimator, and discuss the implications and need for legal clarification.
                        </p>
                     </div>
                  </li>
                  <br/>
                  <li>
                     <a href="https://arxiv.org/pdf/2210.11612.pdf">
                     <b>Searching for a higher power in the human evaluation of MT</b>
                     </a>
                     <br/>
                     <b>Johnny Tian-Zheng Wei</b>,
                     <a href="https://ufal.mff.cuni.cz/tom-kocmi">Tom Kocmi</a>, 
                     and <a href="https://www.microsoft.com/en-us/research/people/chrife/">Christian Federmann</a>.
                     <br/>
                     <b>WMT</b></a> 2022.
                     <br/>
                     [<a href="#" onclick="$('#higher_power_abstract').toggle();return false;">abstract</a>]
                     [<a href="https://aclanthology.org/2022.wmt-1.7.bib">bib</a>]
                     [<a href="https://github.com/johntzwei/interim_sampling">github</a>]
                     <div id="higher_power_abstract" class="abstract" style="display:none;">
                        <p>
                           In MT evaluation, pairwise comparisons are conducted to identify the better system. In conducting the comparison, the experimenter must allocate a budget to collect Direct Assessment (DA) judgments. We provide a cost effective way to spend the budget, but show that typical budget sizes often do not allow for solid comparison. Taking the perspective that the basis of solid comparison is in achieving statistical significance, we study the power (rate of achieving significance) on a large collection of pairwise DA comparisons. Due to the nature of statistical estimation, power is low for differentiating less than 1-2 DA points, and to achieve a notable increase in power requires at least 2-3x more samples. Applying variance reduction alone will not yield these gains, so we must face the reality of undetectable differences and spending increases. In this context, we propose interim testing, an “early stopping” collection procedure that yields more power per judgment collected, which adaptively focuses the budget on pairs that are borderline significant. Interim testing can achieve up to a 27% efficiency gain when spending 3x the current budget, or 18% savings at the current evaluation power.
                        </p>
                     </div>
                  </li>
                  <br/>
                  <li>
                     <a href="https://arxiv.org/pdf/2105.12437">
                     <b>The statistical advantage of automatic NLG metrics at the system level</b>
                     </a>
                     <br/>
                     <b>Johnny Tian-Zheng Wei</b>
                     and <a href="https://robinjia.github.io/">Robin Jia</a>.
                     <br/>
                     <b>ACL</b></a> 2021.
                     <br/>
                     [<a href="#" onclick="$('#statistical_advantage_abstract').toggle();return false;">abstract</a>]
                     [<a href="https://aclanthology.org/2021.acl-long.533.bib">bib</a>]
                     [<a href="https://github.com/johntzwei/metric-statistical-advantage">github</a>]
                     <div id="statistical_advantage_abstract" class="abstract" style="display:none;">
                        <p>
                           Estimating the expected output quality of generation systems is central to NLG. This paper qualifies the notion that automatic metrics are not as good as humans in estimating system-level quality. Statistically, humans are unbiased, high variance estimators, while metrics are biased, low variance estimators. We compare these estimators by their error in pairwise prediction (which generation system is better?) using the bootstrap. Measuring this error is complicated: predictions are evaluated against noisy, human predicted labels instead of the ground truth, and metric predictions fluctuate based on the test sets they were calculated on. By applying a bias-variance-noise decomposition, we adjust this error to a noise-free, infinite test set setting. Our analysis compares the adjusted error of metrics to humans and a derived, perfect segment-level annotator, both of which are unbiased estimators dependent on the number of judgments collected. In MT, we identify two settings where metrics outperform humans due to a statistical advantage in variance: when the number of human judgments used is small, and when the quality difference between compared systems is small.
                        </p>
                     </div>
                  </li>
               </ul>
            </div>
         </div>
         <hr>
         <div class="row" id="publications">
            <div class="col">
               <h2>Policy</h2>
               <ul class="pl">
                  <li>
                     <a href="pdfs/strategic_zero_draft.pdf">
                     <b>A Strategic Zero Draft: Standards for LLM Memorization</b>
                     </a>
                     <br/>
                     <b>Johnny Tian-Zheng Wei</b>
                     and <a href="https://robinjia.github.io/">Robin Jia</a>.
                     <br/>
                     [<a href="https://www.nist.gov/artificial-intelligence/ai-research/nists-ai-standards-zero-drafts-pilot-project-accelerate">call</a>]
                  </li>
               </ul>
            </div>
         </div>
         <hr>
         <div class="row">
            <div class="col">
               <h2>Miscellany</h2>
               <ul>
                  <li>
                     Two books have greatly influenced my perspective on research. They are <a href="https://geekheresy.org/"
                       >Geek Heresy</a> by Kentaro Toyama, and <a href="https://www.youtube.com/watch?v=HiSK7nv-i94"
                       >The Worlds I See</a> by Li Fei-Fei. Thank you both for sharing your stories.
                  </li>
                  <li>
                     I recommend <a href="https://www.gatesnotes.com/Energy/My-new-climate-book-is-finally-here"
                       >How to Avoid a Climate Disaster</a> for everyone, especially if you're an undergraduate or younger.
                  </li>
                  <li>
                     This webpage was adapted from <a href="https://nelsonliu.me"
                       >Nelson's</a>. Thanks!
                  </li>
               </ul>
            </div>
         </div>
         <footer class="pt-2 my-md-2 pt-md-2 border-top">
            <div class="row justify-content-center">
               <div class="col-6 col-md text-left align-self-center">
               </div>
               <div class="col-6 col-md text-right">
                  <a href="https://cl.usc.edu/" class="image-link">
                  <img src="usc-nlp.png" alt="USC NLP logo." height="75">
                  </a>
                  <a href="https://www.usc.edu/" class="image-link">
                  <img src="usc-logo.png" alt="USC logo." height="75">
                  </a>
               </div>
            </div>
         </footer>
      </div>
   </body>
</html>
